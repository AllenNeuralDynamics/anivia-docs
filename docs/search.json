[
  {
    "objectID": "guides_intro.html",
    "href": "guides_intro.html",
    "title": "Guides",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nTitle\n\n\nauthor\n\n\n\n\n\n\nGuide to annotating in 3D\n\n\n \n\n\n\n\nImport and exporting annotations\n\n\n \n\n\n\n\nLabeling multiple animals\n\n\n \n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "tutorial_anipose.html",
    "href": "tutorial_anipose.html",
    "title": "Tutorial: Annotate Anipose format in 3D",
    "section": "",
    "text": "You can find an example fly dataset from google drive. We will use the fly dataset in this tutorial. This dataset is an annotation folder used for tracking flies in Karashchuk et al, “Anipose: A toolkit for robust markerless 3D pose estimation”, Cell Reports, 2021.\n\n\n\n\n\n\nNaming scheme of fly joints in dataset\n\n\n\n\n\nFlies have 6 legs and 4 key joints on each leg. In the annotation, we label the legs L for left, R for right, then 1-3 from front to back. For example, L2 would be the left middle leg.\nThe joints are named A-E corresponding to thorax-coxa joint, coxa-femur joint, femur-tibia joint, tibia-tarsus joint, and tarsus tip. Hence, L2E would be the tarsus tip on the left middle leg.\nSuch condensation of keypoint name is helpful so that labels don’t take up too much space in the interface.\n\n\n\nIf you want another dataset for comparison, you can also find an example mouse annotation dataset here.",
    "crumbs": [
      "Getting started",
      "Tutorial: Annotate Anipose format in 3D"
    ]
  },
  {
    "objectID": "tutorial_anipose.html#an-example-dataset",
    "href": "tutorial_anipose.html#an-example-dataset",
    "title": "Tutorial: Annotate Anipose format in 3D",
    "section": "",
    "text": "You can find an example fly dataset from google drive. We will use the fly dataset in this tutorial. This dataset is an annotation folder used for tracking flies in Karashchuk et al, “Anipose: A toolkit for robust markerless 3D pose estimation”, Cell Reports, 2021.\n\n\n\n\n\n\nNaming scheme of fly joints in dataset\n\n\n\n\n\nFlies have 6 legs and 4 key joints on each leg. In the annotation, we label the legs L for left, R for right, then 1-3 from front to back. For example, L2 would be the left middle leg.\nThe joints are named A-E corresponding to thorax-coxa joint, coxa-femur joint, femur-tibia joint, tibia-tarsus joint, and tarsus tip. Hence, L2E would be the tarsus tip on the left middle leg.\nSuch condensation of keypoint name is helpful so that labels don’t take up too much space in the interface.\n\n\n\nIf you want another dataset for comparison, you can also find an example mouse annotation dataset here.",
    "crumbs": [
      "Getting started",
      "Tutorial: Annotate Anipose format in 3D"
    ]
  },
  {
    "objectID": "tutorial_anipose.html#anipose-annotation-format",
    "href": "tutorial_anipose.html#anipose-annotation-format",
    "title": "Tutorial: Annotate Anipose format in 3D",
    "section": "Anipose annotation format",
    "text": "Anipose annotation format\nThe Anipose annotation format is an extension of the DeepLabCut and Lighting Pose formats to support multiview annotation.\nIf you upgrade anipose to latest version (at least 1.1.2), running anipose extract-frames will now generate this format.\nThe structure of the annotation format:\n\nin main folder (“sarah6_4.8.21_2021-04-28_09-14” in example)\n\nCollectedData.csv - file with annotations\ncalibration.toml - calibration file generated by anipose\nconfig.toml - should have camera cropping offsets (if any)\n\na subfolder for each camera (e.g. “A” in example) containing\n\nimages to annotate - should have same names across camera folders\nanipose_metadata.csv - not used by Anivia, metadata to keep track of frame sources",
    "crumbs": [
      "Getting started",
      "Tutorial: Annotate Anipose format in 3D"
    ]
  },
  {
    "objectID": "tutorial_anipose.html#importing-annotations",
    "href": "tutorial_anipose.html#importing-annotations",
    "title": "Tutorial: Annotate Anipose format in 3D",
    "section": "Importing annotations",
    "text": "Importing annotations\nOpen Anivia, then follow the steps below to import this Anipose dataset:\n\nIn menu: Import &gt; “Load Anipose Dataset”\nSelect the main folder which contains all the camera subfolders: sarah6_4.8.21_2021-04-28_09-14\nClick “upload”\nConfirm you want to “upload” the files\n\nFor Anipose datasets, the annotation is completely local: no files are sent out of your machine.\n\nIn the image above, you can see the different labeled points (L1A, L1B, L1C, etc) in the image.\nTo navigate across images, you can use the left/right arrow keys, clicking in the file menu, or click the left/right arrows in the toolbar.",
    "crumbs": [
      "Getting started",
      "Tutorial: Annotate Anipose format in 3D"
    ]
  },
  {
    "objectID": "tutorial_anipose.html#specifying-bodyparts",
    "href": "tutorial_anipose.html#specifying-bodyparts",
    "title": "Tutorial: Annotate Anipose format in 3D",
    "section": "Specifying bodyparts",
    "text": "Specifying bodyparts\nLet’s look at the bodyparts for this dataset!\nIn the menu, click on: Project &gt; Edit bodyparts\nYou will enter the Bodyparts editor, where you can add or remove bodyparts. It should look like the image below.\n\nIf you have some annotations within your files already, the bodyparts will automatically be populated as in this example. If there are no labels yet, you will need to specify bodyparts.\nYou can drag bodyparts around to change the order for labeling.\nOptionally, you may export the bodyparts to a “bodyparts.toml” file by clicking Export. You can then import this file again to speed up adding bodyparts to a new dataset.",
    "crumbs": [
      "Getting started",
      "Tutorial: Annotate Anipose format in 3D"
    ]
  },
  {
    "objectID": "tutorial_anipose.html#labeling-in-3d",
    "href": "tutorial_anipose.html#labeling-in-3d",
    "title": "Tutorial: Annotate Anipose format in 3D",
    "section": "Labeling in 3D",
    "text": "Labeling in 3D\n\nOpening the 3D interface\nOkay, we’re set up to label. Let’s try labeling in 3D! Click “Toggle 3D annotation” button to enable 3D annotation.\nYou should see the following image: \nYou should see multiple views at the bottom. You can click on another view to jump to that camera.\nNote also the colors of the points! In 3D annotation mode, green means low reprojection error across views (&lt; 2 pixels) and red means high error (&gt; 15 pixels). You may see orange-ish points that have somewhere between 2 and 15 pixels. It’s the mean error across all views, per point. You’ll have to discern which view(s) are causing the issue.\n\n\nAnnotating\nHow to annotate?\nPlace keypoints by clicking in the image. The keypoints will be placed following the order of bodyparts. You can move a keypoint by clicking it and dragging it. Whenever a keypoint is placed or moved, the error is recomputed and all points are recolored.\nTo mark a keypoint as not missing (not visible), place it in within 50 pixels of the top left corner.\nUseful shortcuts:\n\nselect a keypoint and press “d” (or X icon in toolbar) to delete it\nselect all keypoints by pressing “a”\nselect a keypoint and press “m” to mark as missing (moving to top left corner)\n\n\n\nProjecting points\nNow let me introduce you to a feature that feels magical to me.\nIf you have a point labeled in 2 or more views, you can triangulate and project that point onto a new view!\nTo do so, make sure that the point is deleted from the current view (select and press “d” or click the “x” at the top if needed). Then, click the “Project missing points” button. Any point visible in 2+ other views will be projected onto current view!\nI find this useful for placing keypoints initially, especially in views where the pose is hard to annotate. The accuracy of the projection will depend on the accuracy of the labels in other views and the camera calibrations.",
    "crumbs": [
      "Getting started",
      "Tutorial: Annotate Anipose format in 3D"
    ]
  },
  {
    "objectID": "tutorial_anipose.html#exporting",
    "href": "tutorial_anipose.html#exporting",
    "title": "Tutorial: Annotate Anipose format in 3D",
    "section": "Exporting",
    "text": "Exporting\nWhen done, in menu click on: Export &gt; Export Annotations (Anipose format)\nSave the file as “CollectedData.csv” in the main Anipose folder (“sarah6_4.8.21_2021-04-28_09-14” in example).\nYou may need to change settings in your browser to make it prompt where to save your downloaded file.",
    "crumbs": [
      "Getting started",
      "Tutorial: Annotate Anipose format in 3D"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Anivia: Animal image annotator",
    "section": "",
    "text": "Anivia is a new web tool intended specifically for annotation of animal keypoints from images in both 2D and 3D.\nYou can find it here: https://lambdaloop.com/anivia\nKey features:\n\nRuns directly in a browser, no installation needed\nSupport for DeepLabCut, SLEAP, Lightning Pose, and Anipose formats\nImages are loaded in your browser locally\nFor 3D annotation: see reprojection error, project to different viwes, and more!\n\nTo get started, try following one of the tutorials for DeepLabCut (2D) or Anipose (3D) annotation datasets. You may also want check the guides for help with a specific workflow.\n\nAnivia is derived from the VGG Image Annotator (Version 2), by Abhishek Dutta, Akush Gupta, Andrew Zisserman and VIA contributors.",
    "crumbs": [
      "Getting started"
    ]
  },
  {
    "objectID": "guide_import_export.html",
    "href": "guide_import_export.html",
    "title": "Import and exporting annotations",
    "section": "",
    "text": "This is a guide to walk you through the steps of importing and exporting various annotation formats.",
    "crumbs": [
      "Guides",
      "Import and exporting annotations"
    ]
  },
  {
    "objectID": "guide_import_export.html#importing-annotations",
    "href": "guide_import_export.html#importing-annotations",
    "title": "Import and exporting annotations",
    "section": "Importing annotations",
    "text": "Importing annotations\n\nfrom DeepLabCut\nTo import a DeepLabCut dataset, follow the steps below. Note that you can only label one folder at a time.\n\nIn the menu, click on: Import &gt; Load DeepLabCut dataset\nNavigate to your DeepLabCut project\nGo within the labeled-data\nSelect a folder and click “Upload”\n\nIf there are no labels yet, you may need to specify bodyparts (below). Note: Due to no available libraries to parse pytables in javascript, just the h5 file with labels (not images) is sent to server to convert it to a csv to parse.\n\n\nfrom Lightning Pose\nTo import a Lightning Pose dataset, follow the steps below.\n\nIn the menu, click on: Import &gt; Load Lightning Pose dataset\nNavigate to your folder which contains “CollectedData.csv”\nSelect that folder and click “Upload”\n\n\n\nfrom SLEAP\nTo import a SLEAP dataset, follow these steps:\n\nIn the menu, click on: Import &gt; Load SLEAP dataset (SLP file)\nSelect your slp file and select “Upload”\n\nNote that SLP file must have the images padded with zeros to account for the javascript h5 loading library. This is already in the latest SLEAP code and will be available in future SLEAP releases. For now, you can use this python script to pad your video items.",
    "crumbs": [
      "Guides",
      "Import and exporting annotations"
    ]
  },
  {
    "objectID": "guide_import_export.html#exporting-annotations",
    "href": "guide_import_export.html#exporting-annotations",
    "title": "Import and exporting annotations",
    "section": "Exporting annotations",
    "text": "Exporting annotations\n\nto DeepLabCut\nTo export your annotations to DeepLabCut, follow these steps:\n\nIn the menu, click on: Export &gt; Export Annotations (DeepLabCut format)\nSave your file as “CollectedData_SCORER.h5” within the folder within labeled-data that you imported Replace SCORER above with the actual scorer name you specified at the top of your DeepLabCut config.yaml\n\nNote: Similarly to importing, due to limitations on h5 libraries in javascript, the labels are sent to server to convert them to generate an h5 file. Again, no images are ever sent out of your machine, only labels.\n\n\nto Lightning Pose\nTo export your annotations to Lightning Pose, follow these steps:\n\nIn the menu, click on: Export &gt; Export Annotations (Lightning Pose format)\nSave your file as “CollectedData.csv” within the folder you imported\n\n\n\nto SLEAP\nTo export your annotations to Lightning Pose, follow these steps:\n\nIn the menu, click on: Export &gt; Export Annotations (SLP format)\nSave your slp file wherever.\n\nIt might take a bit of time (less than a minute) to generate the SLP file for export.",
    "crumbs": [
      "Guides",
      "Import and exporting annotations"
    ]
  },
  {
    "objectID": "explanation_choices_anivia.html",
    "href": "explanation_choices_anivia.html",
    "title": "Choices made in designing Anivia",
    "section": "",
    "text": "choices to write about\nproject choices\n\nwhy i perceived other tools as insufficient\nextending VGG VIA instead of writing from scratch\n\ndisplay of information\n\nhow to display reprojection error\nUI for projecting points\n\ntechnical choices\n\nimporting files vs server\nserver for deeplabcut loading\nformat for Anipose 3D annotation",
    "crumbs": [
      "Explanation",
      "Choices made in designing Anivia"
    ]
  },
  {
    "objectID": "guide_annotating_in_3d.html",
    "href": "guide_annotating_in_3d.html",
    "title": "Guide to annotating in 3D",
    "section": "",
    "text": "See all the views at once!\nYou can toggle to another view by clicking on it\nPoints are colored by reprojection error: green is low error, red is high error.\n\nIt’s the mean error across all views, per point. You’ll have to discern which view(s) are causing the issue.\n\nProject points based on other views! (this feature feels magical to me)\n\nDelete points you want projected (select and press “d” or click the “x” at the top)\nClick “Project missing points”\nAny point visible in 2+ other views will be projected onto current view",
    "crumbs": [
      "Guides",
      "Guide to annotating in 3D"
    ]
  },
  {
    "objectID": "guide_labeling_multiple_animals.html",
    "href": "guide_labeling_multiple_animals.html",
    "title": "Labeling multiple animals",
    "section": "",
    "text": "Anivia supports labeling multiple animals, but only if you export to a SLEAP file currently.\nThe interface focuses on one animal “instance” at a time. By default when you click to place points it places them in instance 0.\nHere is how you navigate the instance interface:\n\n“i” for new instance (or circle plus in toolbar)\n“u” to delete an instance (or circle minus in toolbar)\nup and down arrows to switch between instances (or circle up and down in toolbar, or just click on a point from that instance)",
    "crumbs": [
      "Guides",
      "Labeling multiple animals"
    ]
  },
  {
    "objectID": "ref_formats.html",
    "href": "ref_formats.html",
    "title": "Supported formats",
    "section": "",
    "text": "TODO: description of each format supported",
    "crumbs": [
      "Reference",
      "Supported formats"
    ]
  },
  {
    "objectID": "ref_formats.html#deeplabcut-format",
    "href": "ref_formats.html#deeplabcut-format",
    "title": "Supported formats",
    "section": "DeepLabCut format",
    "text": "DeepLabCut format",
    "crumbs": [
      "Reference",
      "Supported formats"
    ]
  },
  {
    "objectID": "ref_formats.html#lightning-pose-format",
    "href": "ref_formats.html#lightning-pose-format",
    "title": "Supported formats",
    "section": "Lightning Pose format",
    "text": "Lightning Pose format",
    "crumbs": [
      "Reference",
      "Supported formats"
    ]
  },
  {
    "objectID": "ref_formats.html#sleap-format",
    "href": "ref_formats.html#sleap-format",
    "title": "Supported formats",
    "section": "SLEAP format",
    "text": "SLEAP format",
    "crumbs": [
      "Reference",
      "Supported formats"
    ]
  },
  {
    "objectID": "ref_formats.html#anipose-format",
    "href": "ref_formats.html#anipose-format",
    "title": "Supported formats",
    "section": "Anipose format",
    "text": "Anipose format",
    "crumbs": [
      "Reference",
      "Supported formats"
    ]
  },
  {
    "objectID": "tutorial_deeplabcut.html",
    "href": "tutorial_deeplabcut.html",
    "title": "Tutorial: Annotate a DeepLabCut dataset",
    "section": "",
    "text": "Welcome! In this tutorial, we will import a DeepLabCut dataset, inspect it, test out annotation, and export it again.",
    "crumbs": [
      "Getting started",
      "Tutorial: Annotate a DeepLabCut dataset"
    ]
  },
  {
    "objectID": "tutorial_deeplabcut.html#an-example-dataset",
    "href": "tutorial_deeplabcut.html#an-example-dataset",
    "title": "Tutorial: Annotate a DeepLabCut dataset",
    "section": "An example dataset",
    "text": "An example dataset\nIn this tutorial, we will work with the mouse reaching dataset provided as an example in DeepLabCut repository. You may download the dataset from our mirror on google drive. Alternatively, you may download the DeepLabCut repository from github directly and then navigate to the example in the “examples/Reaching-Mackenzie-2018-08-30”.\nThis dataset is from Mathis et al, “Somatosensory Cortex Plays an Essential Role in Forelimb Motor Adaptation in Mice”, Neuron, 2017.",
    "crumbs": [
      "Getting started",
      "Tutorial: Annotate a DeepLabCut dataset"
    ]
  },
  {
    "objectID": "tutorial_deeplabcut.html#importing-annotations",
    "href": "tutorial_deeplabcut.html#importing-annotations",
    "title": "Tutorial: Annotate a DeepLabCut dataset",
    "section": "Importing annotations",
    "text": "Importing annotations\nOpen Anivia, then follow the steps below to import this DeepLabCut dataset.\n\nIn the menu, click on: Import &gt; Load DeepLabCut dataset\nNavigate to your DeepLabCut project: Reaching-Mackenzie-2018-08-30\nGo within the “labeled-data” folder\nSelect the folder “reachingvideo1” and click “Upload”\nConfirm you want to “upload” the files\n\n\n\n\n\n\n\nNote\n\n\n\nThe upload confirmation just means the folder is sent to the browser, not to a server. Everything is kept locally, with one notable exception below.\nFor the DeepLabCut h5 annotation specifically, the h5 file with labels must be sent to a server to convert it to a csv to parse due to limitations in javascript.\n\n\nIf you follow these steps correctly, your interface should look like the following image.\n\n\n\nThe Anivia interface\n\n\nIn the image above, you can see the different labeled points (Hand, Finger1, Joystick1, Joystick2) in the image.\nTo navigate across images, you can use the left/right arrow keys, clicking in the file menu, or click the left/right arrows in the toolbar.",
    "crumbs": [
      "Getting started",
      "Tutorial: Annotate a DeepLabCut dataset"
    ]
  },
  {
    "objectID": "tutorial_deeplabcut.html#specifying-bodyparts",
    "href": "tutorial_deeplabcut.html#specifying-bodyparts",
    "title": "Tutorial: Annotate a DeepLabCut dataset",
    "section": "Specifying bodyparts",
    "text": "Specifying bodyparts\nLet’s look at the bodyparts for this dataset!\nIn the menu, click on: Project &gt; Edit bodyparts\nYou will enter the Bodyparts editor, where you can add or remove bodyparts. It should look like the image below.\n\nIf you have some annotations within your files already, the bodyparts will automatically be populated as in this example. If there are no labels yet, you will need to specify bodyparts.\nYou can drag bodyparts around to change the order for labeling.\nOptionally, you may export the bodyparts to a “bodyparts.toml” file by clicking Export. You can then import this file again to speed up adding bodyparts to a new dataset.",
    "crumbs": [
      "Getting started",
      "Tutorial: Annotate a DeepLabCut dataset"
    ]
  },
  {
    "objectID": "tutorial_deeplabcut.html#annotating-images",
    "href": "tutorial_deeplabcut.html#annotating-images",
    "title": "Tutorial: Annotate a DeepLabCut dataset",
    "section": "Annotating images",
    "text": "Annotating images\nOkay let’s try to annotate!\nPlace keypoints by clicking in the image. The keypoints will be placed following the order of bodyparts. You can move a keypoint by clicking it and dragging it.\nTo mark a keypoint as not missing (not visible), place it in within 50 pixels of the top left corner.\nUseful shortcuts:\n\nselect a keypoint and press “d” (or X icon in toolbar) to delete it\nselect all keypoints by pressing “a”\nselect a keypoint and press “m” to mark as missing (moving to top left corner)",
    "crumbs": [
      "Getting started",
      "Tutorial: Annotate a DeepLabCut dataset"
    ]
  },
  {
    "objectID": "tutorial_deeplabcut.html#exporting-annotations",
    "href": "tutorial_deeplabcut.html#exporting-annotations",
    "title": "Tutorial: Annotate a DeepLabCut dataset",
    "section": "Exporting annotations",
    "text": "Exporting annotations\nTo export your annotations back, follow these steps:\n\nIn the menu, click on: Export &gt; Export Annotations (DeepLabCut format)\nNavigate to your DeepLabCut project: Reaching-Mackenzie-2018-08-30\nGo back within your labeled-data folder, then within the folder you selected (“reachingvideo1” in this case)\nSave your file as “CollectedData_Mackenzie.h5” within the “reachingvideo1” folder, replacing the current labels\n\nFor your own project, you will need to replace “Mackenzie” with the actual scorer name you specified at the top of your DeepLabCut config.yaml\nThat’s it!\n\n\n\n\n\n\nNote\n\n\n\nSimilarly to importing, due to limitations on h5 libraries in javascript, the labels are sent to server to convert them to generate an h5 file. Again, no images are ever sent out of your machine, only labels.",
    "crumbs": [
      "Getting started",
      "Tutorial: Annotate a DeepLabCut dataset"
    ]
  }
]